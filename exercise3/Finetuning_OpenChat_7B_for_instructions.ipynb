{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "5EFKjtvIjswf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8871ed09-0003-4914-b8ff-18dba199c949"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Using a model from Huggingface (using Transformers)"
      ],
      "metadata": {
        "id": "w9cH-00J6rUx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load model directly\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"openai-community/gpt2\")\n",
        "tokenizer.pad_token_id = 50256\n",
        "model = AutoModelForCausalLM.from_pretrained(\"openai-community/gpt2\")"
      ],
      "metadata": {
        "id": "J7fM7NM_6viH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# let's do inference now\n",
        "\n",
        "while True:\n",
        "    prompt = input(\"Your prompt > \")\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True).to(model.device)\n",
        "    outputs = model.generate(**inputs, max_length=50)  # You can adjust max_length as needed\n",
        "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True, pad_token_id = 50256)\n",
        "    print(generated_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j9Rsdhj362a4",
        "outputId": "23e87219-0c4a-4010-e4f9-084a2c6f04b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "t.co/1QXqYqYqE â€” The Daily Caller (@TheDC) September 24, 2017\n",
            "\n",
            "The Daily Caller reported that the FBI is investigating the Trump campaign's ties to Russia.\n",
            "\n",
            "\"The FBI is\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# OpenChat Finetuning with 4bit Quantization\n",
        "\n",
        "We recommend using a GPU runtime for this example. In the Colab menu bar, choose Runtime > Change Runtime Type and choose GPU under Hardware Accelerator."
      ],
      "metadata": {
        "id": "GhfRuYMVyWh_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install Ludwig\n",
        "\n",
        "We'll use the latest version of Ludwig which includes support for quantized fine-tuning."
      ],
      "metadata": {
        "id": "5tbiuqdEyhj0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QYazZYnssfXQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6349c945-1846-4b4a-d118-460fb197d70b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: Skipping tensorflow as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip uninstall -y tensorflow --quiet\n",
        "!pip install \"ludwig[llm]\" --quiet"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Set up HuggingFace API Token\n",
        "\n",
        "Obtain a [HuggingFace API Token](https://huggingface.co/docs/hub/security-tokens) and request access to [Llama2-7b-hf](https://huggingface.co/meta-llama/Llama-2-7b-hf) before proceeding."
      ],
      "metadata": {
        "id": "D-rbreKEyoeA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"HUGGING_FACE_HUB_TOKEN\"] = \"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\""
      ],
      "metadata": {
        "id": "Y3gUrr7XvHSr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Finetuning our model with Ludwig\n",
        "\n",
        "The Ludwig [configuration](https://ludwig.ai/latest/configuration/) specifies the components of the training job including:\n",
        "\n",
        "- Model type (LLM) and base pretrained model name from HuggingFace\n",
        "- Base model: https://huggingface.co/openchat/openchat_3.5\n",
        "- Input and output features from the training dataset\n",
        "- Quantization (4bit) and parameter-efficient fine-tuning (LoRA)\n",
        "- Training hyperparameters (learning rate, batch size, etc.)\n",
        "- Preprocessing (e.g., sampling to speed up training)\n",
        "- Backend for execution (local, but could also be Ray)"
      ],
      "metadata": {
        "id": "P7q67YNKy7g0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import yaml\n",
        "\n",
        "config_str = \"\"\"\n",
        "model_type: llm\n",
        "base_model: openchat/openchat_3.5\n",
        "\n",
        "quantization:\n",
        "  bits: 4\n",
        "\n",
        "adapter:\n",
        "  type: lora\n",
        "\n",
        "prompt:\n",
        "  template: |\n",
        "    ### Instruction:\n",
        "    {instruction}\n",
        "\n",
        "    ### Input:\n",
        "    {input}\n",
        "\n",
        "    ### Response:\n",
        "\n",
        "input_features:\n",
        "  - name: prompt\n",
        "    type: text\n",
        "    preprocessing:\n",
        "      max_sequence_length: 256\n",
        "\n",
        "output_features:\n",
        "  - name: output\n",
        "    type: text\n",
        "    preprocessing:\n",
        "      max_sequence_length: 256\n",
        "\n",
        "trainer:\n",
        "  type: finetune\n",
        "  learning_rate: 0.0001\n",
        "  batch_size: 1\n",
        "  gradient_accumulation_steps: 16\n",
        "  epochs: 1\n",
        "  learning_rate_scheduler:\n",
        "    warmup_fraction: 0.01\n",
        "\n",
        "preprocessing:\n",
        "  sample_ratio: 0.1\n",
        "\"\"\"\n",
        "\n",
        "config = yaml.safe_load(config_str)"
      ],
      "metadata": {
        "id": "GKie6b70tTkG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train!\n",
        "\n",
        "Start training on your local GPU and monitor progress (including metrics) inline.\n",
        "\n",
        "In this example, we'll be training on the [Alpaca](https://crfm.stanford.edu/2023/03/13/alpaca.html) dataset to turn Llama2-7b into a rudimentary chatbot. But you can use any dataset to fine-tune for other tasks."
      ],
      "metadata": {
        "id": "IgkHvOz7zfIV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "from ludwig.api import LudwigModel\n",
        "\n",
        "\n",
        "model = LudwigModel(config=config, logging_level=logging.INFO)\n",
        "results = model.train(dataset=\"ludwig://alpaca\")\n",
        "print(results)"
      ],
      "metadata": {
        "id": "JfZq1-qbulcg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Deploy our model to HF\n",
        "\n"
      ],
      "metadata": {
        "id": "nlEPUBFaztU0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!huggingface-cli login"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l9fsWqiB531Z",
        "outputId": "2719ce7b-a991-486e-99bf-f18441b89428"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
            "    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
            "    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
            "    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
            "    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
            "\n",
            "    A token is already saved on your machine. Run `huggingface-cli whoami` to get more information or `huggingface-cli logout` if you want to log out.\n",
            "    Setting a new token will erase the existing one.\n",
            "    To login, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n",
            "Token: \n",
            "Add token as git credential? (Y/n) y\n",
            "Token is valid (permission: write).\n",
            "\u001b[1m\u001b[31mCannot authenticate through git-credential as no helper is defined on your machine.\n",
            "You might have to re-authenticate when pushing to the Hugging Face Hub.\n",
            "Run the following command in your terminal in case you want to set the 'store' credential helper as default.\n",
            "\n",
            "git config --global credential.helper store\n",
            "\n",
            "Read https://git-scm.com/book/en/v2/Git-Tools-Credential-Storage for more details.\u001b[0m\n",
            "Token has not been saved to git credential helper.\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ludwig upload hf_hub --repo_id pnotaro/finetuning-test --model_path /content/results/api_experiment_run_5"
      ],
      "metadata": {
        "id": "Vud-zk1Yl8ri",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "908dc667-85f3-4f6c-9587-05d36030bc56"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "adapter_model.safetensors: 100% 13.6M/13.6M [00:02<00:00, 5.17MB/s]\n",
            "Model uploaded to `https://huggingface.co/pnotaro/finetuning-test/tree/main/` with repository name `pnotaro/finetuning-test`\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create an OpenAI compatible API for your model"
      ],
      "metadata": {
        "id": "sgytfD6V-Oaa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install easyllm"
      ],
      "metadata": {
        "id": "V08cNEvS-bK4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from easyllm.clients import huggingface\n",
        "from easyllm.prompt_utils import build_llama2_prompt\n",
        "\n",
        "# helper to build llama2 prompt\n",
        "huggingface.prompt_builder = build_llama2_prompt\n",
        "\n",
        "response = huggingface.ChatCompletion.create(\n",
        "    model=\"pnotaro/finetuning-test\",\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": \"\\nYou are a helpful assistant speaking like a pirate. argh!\"},\n",
        "        {\"role\": \"user\", \"content\": \"What is the sun?\"},\n",
        "    ],\n",
        "      temperature=0.9,\n",
        "      top_p=0.6,\n",
        "      max_tokens=256,\n",
        ")\n",
        "\n",
        "print(response)"
      ],
      "metadata": {
        "id": "0V-DmCaG-UyS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}